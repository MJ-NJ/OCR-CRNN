{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mukti\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "C:\\Users\\mukti\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torchvision\\io\\image.py:13: UserWarning: Failed to load image Python extension: '[WinError 127] The specified procedure could not be found'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0123456789.-ABCDEFGHIJKLMNOPQRSTUVWXYZ/\n",
      "{' ': 0, '0': 1, '1': 2, '2': 3, '3': 4, '4': 5, '5': 6, '6': 7, '7': 8, '8': 9, '9': 10, '.': 11, '-': 12, 'A': 13, 'B': 14, 'C': 15, 'D': 16, 'E': 17, 'F': 18, 'G': 19, 'H': 20, 'I': 21, 'J': 22, 'K': 23, 'L': 24, 'M': 25, 'N': 26, 'O': 27, 'P': 28, 'Q': 29, 'R': 30, 'S': 31, 'T': 32, 'U': 33, 'V': 34, 'W': 35, 'X': 36, 'Y': 37, 'Z': 38, '/': 39}\n"
     ]
    }
   ],
   "source": [
    "with open('alphabet.txt') as f:\n",
    "    alphabet = f.readline()\n",
    "print(alphabet)    \n",
    "\n",
    "# Map the characters in the alphabet to the index\n",
    "alphabet_map = {}\n",
    "for i, char in enumerate(alphabet):\n",
    "    alphabet_map[char] = i\n",
    "print(alphabet_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start process train data\n",
      "image count: 2906 , label count: 2906\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2906/2906 [00:34<00:00, 83.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start process test data\n",
      "image count: 971 , label count: 971\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 971/971 [00:11<00:00, 83.73it/s]\n"
     ]
    }
   ],
   "source": [
    "def pre_process_img_file(label_file, img_dir, new_dir):\n",
    "\n",
    "    img_names = []\n",
    "    labels = []\n",
    "    with open(label_file, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            img_names.append(line.strip('\\n').split(' ')[0].split('/')[1]) \n",
    "            idxs = line.strip('\\n').split(' ')[1:]\n",
    "            labels.append(''.join([alphabet[int(idx)] for idx in idxs]))\n",
    "    print('image count:', len(img_names), ', label count:', len(labels))\n",
    "\n",
    "    if os.path.exists(new_dir):\n",
    "        shutil.rmtree(new_dir)\n",
    "\n",
    "    os.mkdir(new_dir)\n",
    "\n",
    "    for idx, img_name in enumerate(tqdm(img_names)):\n",
    "        img_path = os.path.join(img_dir, img_name)\n",
    "        new_path = os.path.join(new_dir, img_name.split('_')[0] + '_' + labels[idx] + '.jpg')\n",
    "        shutil.copyfile(img_path, new_path)\n",
    "           \n",
    "print('start process train data')\n",
    "pre_process_img_file(label_file='data_train.txt',\n",
    "                     img_dir='train_imgs',\n",
    "                     new_dir='project/train/')\n",
    " \n",
    "print('start process test data')\n",
    "pre_process_img_file(label_file='data_test.txt',\n",
    "                     img_dir='test_imgs',\n",
    "                     new_dir='project/test/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, data_dir):\n",
    "        \"\"\"Inits dataset\"\"\"\n",
    "        self.data_dir = data_dir\n",
    "        self.alphabet_map = alphabet_map\n",
    "        self.img_names = os.listdir(self.data_dir)\n",
    "        self.labels = [i.split('_')[1].split('.')[0] for i in self.img_names]\n",
    "        self.trans = transforms.Compose([\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Get single image by idx\n",
    "        \n",
    "        Args:\n",
    "            idx: index\n",
    "            \n",
    "        Returns:\n",
    "            img: torch.FloatTensor\n",
    "            label: Actual lable of the image, like \"ZOW-PRF-LFB\".\n",
    "        \"\"\"\n",
    "        img_path = os.path.join(self.data_dir, self.img_names[idx])\n",
    "        img = Image.open(img_path)\n",
    "        img = self.trans(img)\n",
    "        label = self.labels[idx]\n",
    "        return img, label\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTM(nn.Module):\n",
    "    \"\"\" Bidirectional LSTM and embedding layer.\n",
    "    \n",
    "    Attributes:\n",
    "        rnn: Bidirectional LSTM\n",
    "        linear: Embedding layer\n",
    "    \"\"\"\n",
    "    def __init__(self, num_input, num_hiddens, num_output):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.LSTM(num_input, num_hiddens, bidirectional=True)\n",
    "        # the size of input of embedding layer should mutiply by 2, because of the bidirectional.\n",
    "        self.linear = nn.Linear(num_hiddens * 2, num_output)  \n",
    "    \n",
    "    def forward(self, X):\n",
    "        rnn_out, _ = self.rnn(X)\n",
    "        T, b, h = rnn_out.size()  # T: time step, b: batch size, h: hidden size\n",
    "        t_rec = rnn_out.view(T * b, h)\n",
    "        output = self.linear(t_rec)\n",
    "        output = output.view(T, b, -1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CRNN(nn.Module):\n",
    "   \n",
    "    def __init__(self, num_class):\n",
    "        super().__init__()\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False),\n",
    "            nn.Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False),\n",
    "            nn.Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "            nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 1), padding=(0, 1), dilation=1, ceil_mode=False),\n",
    "            nn.Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "            nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 1), padding=(0, 1), dilation=1, ceil_mode=False),\n",
    "            nn.Conv2d(512, 512, kernel_size=(2, 2), stride=(1, 1)),\n",
    "            nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.rnn = nn.Sequential(\n",
    "            BiLSTM(512, 256, 256),\n",
    "            BiLSTM(256, 256, num_class)\n",
    "        )\n",
    "        \n",
    "    def forward(self, X):\n",
    "        cnn_out = self.cnn(X)  # cnn_out shape: (batch_size x channel x height x width)\n",
    "        assert cnn_out.shape[2] == 1, \"the height of conv must be 1\"\n",
    "        cnn_out = cnn_out.squeeze(2)  # squeeze the dim 2 (height) of cnn_out\n",
    "        cnn_out = cnn_out.permute(2, 0, 1)  # move the width to the first dim, as the time step of rnn input\n",
    "        output = self.rnn(cnn_out)  # output shape: (time step x batch_size x num_class)\n",
    "        output = F.log_softmax(output, dim=2)  # do softmax at the dim of num_class\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape: torch.Size([64, 1, 32, 200])\n",
      "output shape from CRNNnet: torch.Size([51, 64, 40])\n"
     ]
    }
   ],
   "source": [
    "train_set = MyDataset(data_dir='project/train/')\n",
    "batch_size = 64\n",
    "trainloader = DataLoader(dataset=train_set, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "# Check if the input and output shapes meet expectations\n",
    "for X, y in trainloader:\n",
    "    break\n",
    "print('input shape:', X.shape)\n",
    "crnn = CRNN(num_class=len(alphabet))\n",
    "preds = crnn(X)\n",
    "print('output shape from CRNNnet:', preds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ctcloss_parameters(text_batch):\n",
    "    actual_length = []\n",
    "    result = []\n",
    "    for item in text_batch:            \n",
    "        actual_length.append(len(item))\n",
    "        r = []\n",
    "        for char in item:\n",
    "            index = alphabet_map[char]\n",
    "            r.append(index)\n",
    "        result.append(r)\n",
    "\n",
    "    max_len = 0\n",
    "    for r in result:\n",
    "        if len(r) > max_len:\n",
    "            max_len = len(r)\n",
    "\n",
    "    result_temp = []\n",
    "    for r in result:\n",
    "        for i in range(max_len - len(r)):\n",
    "            r.append(0)\n",
    "        result_temp.append(r)\n",
    "\n",
    "    encoded_text = result_temp\n",
    "    encoded_text = torch.LongTensor(encoded_text)\n",
    "    preds_length = torch.LongTensor([preds.size(0)] * batch_size)\n",
    "    actual_length = torch.LongTensor(actual_length)\n",
    "    return encoded_text, preds_length, actual_length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1          loss: 0.054365\n",
      "epoch 2          loss: 0.054002\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\Projects\\ai_project\\main.ipynb Cell 9\u001b[0m in \u001b[0;36m<cell line: 17>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects/ai_project/main.ipynb#X11sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m preds \u001b[39m=\u001b[39m crnn(X) \n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects/ai_project/main.ipynb#X11sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m encoded_text, preds_length, actual_length \u001b[39m=\u001b[39m get_ctcloss_parameters(y)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Projects/ai_project/main.ipynb#X11sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m encoded_text \u001b[39m=\u001b[39m encoded_text\u001b[39m.\u001b[39;49mto(device)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects/ai_project/main.ipynb#X11sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m preds_length \u001b[39m=\u001b[39m preds_length\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects/ai_project/main.ipynb#X11sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m actual_length \u001b[39m=\u001b[39m actual_length\u001b[39m.\u001b[39mto(device)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "use_gpu = True\n",
    "num_epoch = 100\n",
    "\n",
    "if use_gpu:\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "# device = torch.device('cpu')\n",
    "    \n",
    "crnn.train()\n",
    "trainer = torch.optim.Adam(crnn.parameters(), lr=0.001)\n",
    "loss = nn.CTCLoss(zero_infinity=True)\n",
    "crnn = crnn.to(device)\n",
    "loss = loss.to(device)\n",
    "    \n",
    "for epoch in range(num_epoch):\n",
    "    for X, y in trainloader:\n",
    "        X = X.to(device)\n",
    "        trainer.zero_grad()\n",
    "        preds = crnn(X) \n",
    "        encoded_text, preds_length, actual_length = get_ctcloss_parameters(y)\n",
    "\n",
    "        encoded_text = encoded_text.to(device)\n",
    "        \n",
    "        preds_length = preds_length.to(device)\n",
    "        actual_length = actual_length.to(device)\n",
    "        \n",
    "        l = loss(preds, encoded_text,preds_length, actual_length) / batch_size\n",
    "        l.backward()\n",
    "        trainer.step()\n",
    "    print('epoch', str(epoch + 1).ljust(10), 'loss:', format(l.item(), '.6f'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_final_pred(text):\n",
    "    text = list(text)\n",
    "    for i in range(len(text)):\n",
    "        for j in range(i + 1, len(text)):\n",
    "            if text[j] == ' ':\n",
    "                break\n",
    "            else:\n",
    "                if text[j] == text[i]:\n",
    "                    text[j] = ' '\n",
    "                else:\n",
    "                    continue\n",
    "    final_text = ''.join(text).replace(' ', '')\n",
    "    return final_text\n",
    "\n",
    "def predict(net, X, y):\n",
    "    preds = net(X)\n",
    "    _, preds = preds.max(2)\n",
    "    idx = 0\n",
    "    print('crnn net output'.ljust(51), '|', 'final predict'.ljust(20), '|', 'ground truth'.ljust(20))\n",
    "    print('=' * 99)\n",
    "    for pred in preds.permute(1, 0):\n",
    "        pred_text = ''.join([alphabet[i.item()] for i in pred])\n",
    "        print(pred_text, '|', get_final_pred(pred_text).ljust(20), '|', y[idx].ljust(20))\n",
    "        print('·' * 99)\n",
    "        idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MyDataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32md:\\Projects\\ai_project\\main.ipynb Cell 11\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Projects/ai_project/main.ipynb#X14sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m test_set \u001b[39m=\u001b[39m MyDataset(data_dir\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m/kaggle/working/test/\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Projects/ai_project/main.ipynb#X14sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# predict single image with random index\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Projects/ai_project/main.ipynb#X14sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m idx \u001b[39m=\u001b[39m random\u001b[39m.\u001b[39mrandint(\u001b[39m0\u001b[39m, \u001b[39mlen\u001b[39m(test_set) \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'MyDataset' is not defined"
     ]
    }
   ],
   "source": [
    "test_set = MyDataset(data_dir='project/test/')\n",
    "\n",
    "# predict single image with random index\n",
    "idx = random.randint(0, len(test_set) - 1)\n",
    "X, y = test_set[idx]\n",
    "X = X.unsqueeze(0) # add dim as batch\n",
    "y = [y]\n",
    "X = X.to(device)\n",
    "predict(crnn, X, y)\n",
    "print('\\n' * 2)\n",
    "# predict batch using dataloader\n",
    "testloader = DataLoader(test_set, batch_size=8, shuffle=True, drop_last=True)\n",
    "X, y = next(iter(testloader))\n",
    "X = X.to(device)\n",
    "predict(crnn, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
